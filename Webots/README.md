This directory contains a Webots world and a controller for interfacing with a UR5 robot arm and its sensors. 

The world simulates the canonical block world from classical AI. The intent is for the UR5 to rearrange the blocks based on a task plan generated from PDDL based on a semantic world model. The blocks are localized using depth & color images generated by the cameras in the gripper palm.

I (Dylan) never incorporated the webots simulation with the full task-planning perception architecture in "Full-System".
It could be ported. However, in ../TaskPlanner-Perception-Example there is a jupyter notebook that runs the task planner based on saved color, depth, and segmentation images generated from webots and generates a sequence of coordinates to move to. It is useful for testing system setup and could be intergrated with this controller. However, that task planning pipeline is not as mature as what is in ../Full-System and could be updated.

I ran into issues using jupyter notebooks with Webots so integrating the jupyter notebook with Webots may require using regular .py files which have worked with Webots.

The controller here is an interface that enables:
- Moving the UR5's end-effector to poses via MoveL and MoveJ primitives
- Taking depth, color, and segmentation images using the rangefinder and rgb camera in the palm and displaying them in matplotlib and saving them as numpy arrays for analysis

Future work might include:
- Integrating the Perception class/system with YOLOV8 perception in "Full-System" rather than relying on Webots segmentation.
- Integrating the Py2PDDL-based task planning system in "Full System".
- Using the robotics toolbox for python and spatial math as the kinematics backend since it has more features and is more robust than what is here.
