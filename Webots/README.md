This directory contains a Webots world and a controller for interfacing with a UR5 robot arm and it's sensors. 

The world simulates the canonical block world from classical AI. The intent is for the UR5 to rearrange the blocks based on a task plan generated from PDDL based on a semantic world model. The blocks are localized using depth & color images generated by the cameras in the gripper palm.

I (Dylan) never incorporated the webots simulation with the full task-planning perception architecture in "Full-System".
It could be ported. 

I ran into issues using jupyter notebooks with Webots so it may need to be written in regular .py files which have worked with Webots.

The controller here is an interface that enables:
- Moving the UR5's end-effector to poses via MoveL and MoveJ primitives
- Taking depth, color, and segmentation images using the rangefinder and rgb camera in the palm and displaying them in matplotlib and saving them as numpy arrays for analysis

Future work might include:
- Integrating the Perception class/system with YOLOV8 perception in "Full-System" rather than relying on Webots segmentation.
- Integrating the Py2PDDL-based task planning system in "Full System".
- Using the robotics toolbox for python and spatial math as the kinematics backend since it has more features and is more robust than what is here.
